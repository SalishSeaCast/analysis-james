{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import netCDF4 as nc\n",
    "import seaborn as sns\n",
    "import xarray as xr\n",
    "from salishsea_tools import geo_tools, geo_tools, tidetools\n",
    "import functools\n",
    "from IPython.display import clear_output\n",
    "import datetime\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['image.cmap'] = 'jet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Function used to get chlorophyll data from text files produced in 2003, 2004, 2005 cruises\n",
    "def extract_data(file_path):\n",
    "    file_description = []\n",
    "    default_cols = [\"STATION\", \"DATE\", \"TIME\", \"LAT\", \"LON\"]\n",
    "    time_loc_data = 5*[\"NaN\"]\n",
    "    data_list = []\n",
    "    with open(file_path) as f:\n",
    "            indata = False\n",
    "            for line in f:\n",
    "                if not indata:\n",
    "                    if line.startswith(\"%\"):\n",
    "                        file_description.append(line)\n",
    "                    elif line.startswith(\"*\"):\n",
    "                        split_line = line.split()\n",
    "                        columns = split_line[1:len(split_line)]\n",
    "                        num_cols = len(columns)\n",
    "                        indata = True\n",
    "                else:\n",
    "                    split_line = line.split()\n",
    "                    if not line.startswith(\"S\") and not line.startswith(\"T\") and not line.startswith(\"F\") and not line.startswith(\"P\") and len(split_line) == num_cols:\n",
    "                        data_list.append(time_loc_data + split_line)\n",
    "                    elif len(split_line) > 0:\n",
    "                        try:\n",
    "                            station = split_line[0]\n",
    "                        except:\n",
    "                            station = \"NaN\"\n",
    "                        try:\n",
    "                            date = split_line[2] + \" \" + split_line[3] + \" \" + split_line[4]\n",
    "                            date = pd.to_datetime(date, infer_datetime_format=True)\n",
    "                        except:\n",
    "                            date = pd.to_datetime(\"NaN\", infer_datetime_format=True, errors = \"coerce\")\n",
    "                        try:\n",
    "                            time = split_line[5]\n",
    "                        except:\n",
    "                            time = \"NaN\"\n",
    "                        try:\n",
    "                            lat = split_line[6] + \" \" + split_line[7]\n",
    "                            lon = split_line[9] + \" \" + split_line[10]\n",
    "                        except:\n",
    "                            lat, lon = \"NaN\", \"NaN\"\n",
    "                        time_loc_data = [station,date,time,lat,lon]\n",
    "            \n",
    "    return(pd.DataFrame(data_list, columns = default_cols + columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Iterate through top level chlorophyll data directory, grabbing data from applicable files\n",
    "\n",
    "basedir = \"/ocean/shared/SoG/btl/\"\n",
    "chl_dict = dict()\n",
    "\n",
    "for subdir in os.listdir(basedir):\n",
    "    if os.path.isdir(basedir + '/' + subdir):\n",
    "        for file in os.listdir(basedir + '/' + subdir):\n",
    "            if file.startswith(\"bottle\") and file.endswith(\".txt\"):\n",
    "                chl_dict[subdir] = extract_data(basedir + \"/\" + subdir + \"/\" + file)\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convert from sexagesimal lat or lon string to decimal lat or lon float\n",
    "def strToLatLon(s):\n",
    "    try:\n",
    "        split_str = s.split()\n",
    "        return(float(split_str[0]) + float(split_str[1])/60)\n",
    "    except:\n",
    "        return(np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Convert chlorophyll dataframe column types\n",
    "\n",
    "all_chl = pd.concat(chl_dict, join = \"inner\")\n",
    "all_chl.reset_index(inplace = True)\n",
    "\n",
    "all_chl[\"DEPTH\"] = pd.to_numeric(all_chl[\"depth\"], errors='coerce')\n",
    "all_chl[\"chl002\"] = pd.to_numeric(all_chl[\"chl002\"], errors='coerce')\n",
    "all_chl[\"chl020\"] = pd.to_numeric(all_chl[\"chl020\"], errors='coerce')\n",
    "all_chl[\"chl200\"] = pd.to_numeric(all_chl[\"chl200\"], errors='coerce')\n",
    "\n",
    "all_chl[\"DECIMAL_LAT\"] = (all_chl[\"LAT\"].apply(strToLatLon))\n",
    "all_chl[\"DECIMAL_LON\"] = (all_chl[\"LON\"].apply(strToLatLon))\n",
    "all_chl[\"DECIMAL_LON\"] = -all_chl[\"DECIMAL_LON\"] # needs to be negative to match grid lon/lats\n",
    "all_chl[\"STATION_LAT\"] =  all_chl.groupby(\"STATION\")[\"DECIMAL_LAT\"].transform(np.median)\n",
    "all_chl[\"STATION_LON\"] =  all_chl.groupby(\"STATION\")[\"DECIMAL_LON\"].transform(np.median)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create smaller data frame with location of stations\n",
    "station_lon_lat = all_chl[[\"STATION\", \"STATION_LON\", \"STATION_LAT\"]].drop_duplicates()\n",
    "station_lon_lat.columns = [\"STATION\", \"LON\", \"LAT\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Add corresponding model points to station data frame\n",
    "grid_B = nc.Dataset('/data/nsoontie/MEOPAR/NEMO-forcing/grid/bathy_meter_SalishSea2.nc')\n",
    "bathy, X, Y = tidetools.get_bathy_data(grid_B)\n",
    "\n",
    "def closest_model_point_wrapper(lon, lat):\n",
    "    try:\n",
    "        model_point = geo_tools.find_closest_model_point(lon, lat, model_lons = X, model_lats = Y, grid = 'NEMO', land_mask = bathy.mask)\n",
    "        return(model_point)\n",
    "    except:\n",
    "        print(\"ERROR\" + str(lon) + \" \" + str(lat))\n",
    "        return(np.nan)\n",
    "    \n",
    "station_lon_lat[\"MODEL_POINT\"] = station_lon_lat.apply(lambda row: closest_model_point_wrapper(row[1], row[2]) , axis = 1)\n",
    "station_lon_lat[\"MODEL_J\"] = station_lon_lat[\"MODEL_POINT\"].apply(lambda x: x[0])\n",
    "station_lon_lat[\"MODEL_I\"] = station_lon_lat[\"MODEL_POINT\"].apply(lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Choose which values to add to nowcast dataframe\n",
    "\n",
    "tracers = [\"PHY2\"]\n",
    "\n",
    "plot_months = [\"mar\"]\n",
    "\n",
    "plot_hours = np.array([0, 12, 18])\n",
    "\n",
    "max_depth = 20\n",
    "result_depths = xr.open_dataset('/data/nsoontie/MEOPAR/NEMO-forcing/grid/deptht_428m.nc').deptht.values\n",
    "depth_indices = np.where(result_depths < max_depth)[0]\n",
    "\n",
    "model_points = station_lon_lat[\"MODEL_POINT\"]\n",
    "model_js = [x[0] for x in model_points] \n",
    "model_is = [x[1] for x in model_points] \n",
    "\n",
    "stations = station_lon_lat[\"STATION\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Iterate through nowcast green results, grabbing certain tracers, locations, and dates/times\n",
    "# Create pandas dataframe and save result\n",
    "\n",
    "load_new_dataset = False\n",
    "\n",
    "if load_new_dataset:\n",
    "    nowcast_dir = \"/results/SalishSea/nowcast-green/\" #\"/data/jpetrie/MEOPAR/SalishSea/results/nowcast_results/\"\n",
    "\n",
    "    month_num = {\"jan\": \"01\",\"feb\": \"02\", \"mar\": \"03\", \"apr\": \"04\", \"may\": \"05\", \"jun\": \"06\", \"jul\": \"07\", \"aug\": \"08\", \"sep\": \"09\", \"oct\": \"10\", \"nov\": \"11\", \"dec\": \"12\" }\n",
    "\n",
    "    dataframe_list = []\n",
    "    num_files = 0\n",
    "    start_time = datetime.datetime.now()\n",
    "    for subdir in os.listdir(nowcast_dir):\n",
    "        if os.path.isdir(nowcast_dir + '/' + subdir) and re.match(\"[0-9]{2}[a-z]{3}[0-9]{2}\", subdir):\n",
    "            month_str = subdir[2:5]\n",
    "            date_str = \"20\" + subdir[5:7] + month_num[month_str] + subdir[0:2]\n",
    "            tracer_file = \"SalishSea_1h_\" + date_str + \"_\" + date_str + \"_ptrc_T.nc\"\n",
    "            tracer_path = nowcast_dir + \"/\" + subdir + \"/\" + tracer_file\n",
    "            if os.path.isfile(tracer_path) and month_str in plot_months:\n",
    "                grid_t = xr.open_dataset(tracer_path)\n",
    "                result_hours = pd.DatetimeIndex(grid_t.time_centered.values).hour\n",
    "                time_indices = np.where([(x in plot_hours) for x in result_hours])\n",
    "                \n",
    "                J, T, Z = np.meshgrid(model_js,time_indices,depth_indices, indexing = 'ij')\n",
    "                I, T, Z = np.meshgrid(model_is,time_indices,depth_indices, indexing = 'ij')\n",
    "                \n",
    "                tracer_dataframes = []\n",
    "                for t in tracers:\n",
    "                    station_slice = grid_t[t].values[T,Z,J,I]\n",
    "                    slice_xarray = xr.DataArray(station_slice,\n",
    "                                     [stations,result_hours[time_indices], result_depths[depth_indices]],\n",
    "                                     [\"STATION\", \"HOUR\", \"DEPTH\"], \n",
    "                                     t)\n",
    "                    slice_dataframe = slice_xarray.to_dataframe()\n",
    "                    slice_dataframe.reset_index(inplace = True)\n",
    "                    tracer_dataframes.append(slice_dataframe)\n",
    "                merged_tracers = functools.reduce(lambda left,right: pd.merge(left,right,on=[\"STATION\", \"HOUR\", \"DEPTH\"]), tracer_dataframes)\n",
    "                merged_tracers[\"DATE\"] = date_str\n",
    "                merged_tracers[\"MONTH\"] = int(month_num[month_str])\n",
    "                dataframe_list.append(merged_tracers)\n",
    "\n",
    "                num_files = num_files + 1\n",
    "                run_time = datetime.datetime.now() - start_time\n",
    "                clear_output()\n",
    "                print(\"Files loaded:\" + str(num_files))\n",
    "                print(\"Date of most recent nowcast load: \" + date_str)\n",
    "                print(\"Time loading: \")\n",
    "                print(run_time)\n",
    "                print(\"\\n\\n\\n\")\n",
    "                print(merged_tracers)\n",
    "\n",
    "    nowcast_df = pd.concat(dataframe_list)    \n",
    "    t = datetime.datetime.now()\n",
    "    time_string = str(t.year) +\"_\"+ str(t.month) +\"_\"+ str(t.day) +\"_\"+ str(t.hour) +\"_\"+ str(t.minute)\n",
    "    nowcast_df.to_pickle(\"/ocean/jpetrie/MEOPAR/analysis-james/nowcast_green_subset/\"+ time_string + \".pkl\") \n",
    "    \n",
    "else: \n",
    "    past_dataset_path = \"/ocean/jpetrie/MEOPAR/analysis-james/nowcast_green_subset/2016_8_3_16_58.pkl\"\n",
    "    nowcast_df = pd.read_pickle(past_dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nowcast_df[\"DAY_OF_MONTH\"] = pd.to_numeric(nowcast_df[\"DATE\"])%100\n",
    "nowcast_df[\"MONTH\"] = ((pd.to_numeric(nowcast_df[\"DATE\"])%10000))//100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
